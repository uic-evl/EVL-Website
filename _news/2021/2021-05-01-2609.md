---
layout: post
title: '"Recursive meta-Reinforcement Learning for Personalized Sequential Dynamic Treatment Policies"'
date: 2021-05-01
tags: paper
categories: papers
tabs: true
image: etardini_thesis_2021.png-srcw.jpg
---

## Recursive meta-Reinforcement Learning for Personalized Sequential Dynamic Treatment Policies
**Tardini, E.**
- Publication: Graduate College of the University of Illinois at Chicago
- PDF: [etardini_thesis_2021.pdf](/documents/etardini_thesis_2021.pdf)


[![image](https://www.evl.uic.edu/output/originals/etardini_thesis_2021.png-srcw.jpg){:style="max-width: 100%"}](https://www.evl.uic.edu/output/originals/etardini_thesis_2021.png-srcw.jpg)
Main features, outcomes, and decisions of the OPC treatment sequence

In recent years deep meta-reinforcement learning has extended the applicability of reinforcement learning (RL) algorithms: by integrating recurrent networks, trained models have the ability to quickly adapt to new unseen environments without the need for further backpropagation. These models, however, cannot adapt without having information on past rewards, and are therefore not directly applicable to a sequential decision-making setting in which multiple steps are required before observing the final reward.<br><br>
One of the main applications affected by this limitation are dynamic treatment regimes, i.e. the problem of selecting the optimal medical treatment sequence for a patient at each step, keeping into account the complete past treatment history. By expanding deep meta-reinforcement learning to handle sequential decisions, a model would be able to prescribe the optimal treatment for each patient even if the patient&rsquo;s (or physician&rsquo;s) preferences on the outcome were never encountered by the model in training.<br><br>
We propose a recursive deep meta-reinforcement learning approach which enables the model of each decision of the sequential process to learn from and adapt to unseen circumstances by recursively integrating the feedback of the models of other decisions in the process. We evaluate our approach on synthetic two-step processes with fixed transition probabilities but varying reward functions, to test the models' ability to propagate environment information from the final reward to intermediate steps. Finally, we train our model on a dataset of three-step chemo-radiotherapeutic and surgical treatment of oropharyngeal squamous cell carcinoma patients, proving our approach&rsquo;s ability to optimally handle previously unseen patient's preferences on survival and toxicity outcomes.