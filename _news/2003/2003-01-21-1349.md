---
layout: post
title: '"Real-time Camera-based Face Detection using a Modified LAMSTAR Neural Network System"'
date: 2003-01-21
tags: paper
categories: papers
tabs: true
image: facetrackerscreenshot.png-srcw.jpg
---

## Real-time Camera-based Face Detection using a Modified LAMSTAR Neural Network System
**Girado, J., Sandin, D., DeFanti, T., Wolf, L.**
- Publication: San Jose, California
- Publication: www.spie.org/web/meetings/programs/pw01/confs/4305.html
- PDF: [girado_lamstar.pdf](/documents/girado_lamstar.pdf)


[![image](https://www.evl.uic.edu/output/originals/facetrackerscreenshot.png-srcw.jpg){:style="max-width: 100%"}](https://www.evl.uic.edu/output/originals/facetrackerscreenshot.png-srcw.jpg)
Screen Shot of Face Detection
Credit: J. Girado, EVL

This paper describes a cost-effective, real-time (640x480 at 30Hz) upright frontal face detector as part of an ongoing project to develop a video-based, tetherless 3D head position and orientation tracking system. The work is specifically targeted for auto-stereoscopic displays and projection-based virtual reality systems. The proposed face detector is based on a modified LAMSTAR neural network system. At the input stage, after achieving image normalization and equalization, a sub-window analyzes facial features using a neural network. The sub-window is segmented, and each part is fed to a neural network layer consisting of a Kohonen Self-Organizing Map (SOM). The output of the SOM neural networks are interconnected and related by correlation-links, and can hence determine the presence of a face with enough redundancy to provide a high detection rate. To avoid tracking multiple faces simultaneously, the system is initially trained to track only the face centered in a box superimposed on the display. The system is also rotationally and size invariant to a certain degree.<br><br>

The Electronic Visualization Laboratory (EVL) is one of several research groups working on producing PC-driven, projection-based virtual reality (VR) displays. The use of high-end systems, such as EVL&rsquo;s CAVE and ImmersaDesk are well established in application areas such as computational science, automotive engineering and chemical exploration. The next-generation of VR displays, both tiled LCD displays and projection-based, aim to eliminate encumbrances on the user. The trend is towards higher resolution displays where the user is not required to wear special glasses to view stereoscopic scenes. Analogously, the trend for interaction with these displays is towards lightweight and tetherless input-devices.<br><br>

EVL and its collaborators are exploring the use of other modalities (such as vision, speech and gesture) as humancomputer interfaces for this new generation of VR systems. Gesture recognition can come from either tracking the user&rsquo;s movements or processing them using video camera input. Gaze direction, or eye tracking, using camera input is also possible. Audio support can be used for voice recognition and generation, as well as used in conjunction with recording tele-immersive sessions. Used together, these systems enable tetherless tracking and unencumbered hand movements for improved interaction and collaboration within the virtual scene.<br><br>

The research described here is a cost-effective real-time (640x480 at 30Hz) face detector that will serve as the core of a video-based, tetherless 3D head position and orientation tracking system targeted for either auto-stereoscopic displays or projection-based virtual reality systems. It will be tested first using EVL&rsquo;s Access Grid Augmented / Autostereo Virtual Environment (AGAVE), a high-resolution autostereoscopic display consisting of tiled LCD displays driven by a PC cluster and fitted with a highly sensitive tracking system to track user&rsquo;s gaze and gestures without the use of head mounted or hand held tracking devices.<br><br>

The complete tracking system will consist of two neural network-based face detectors working in parallel, each running through four distinct phases. They are: pre-processing, which includes input masking, image normalization, histogram equalization, and image sub-sampling; detection, where a modified LAMSTAR neural network takes the pre-processed image, scans for a face and outputs the coordinates of the corresponding box surrounding the face; postprocessing, where facial feature extraction and stereo correspondence matching occurs to extract the 3D information; and the implementation of a prediction module, which is based on a neural network linear filter with a Tap Delay Line (TDL). The function of the prediction module is to inform the face detection modules where in the scene the face will likely be found, so as to avoid scanning the next whole frame for a face. If the face is not detected in the predicted position, the system will rescan the entire scene. Phases one and three use computer vision techniques. This paper addresses the pre-processing and detection phases.