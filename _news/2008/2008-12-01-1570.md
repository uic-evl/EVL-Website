---
layout: post
title: '"Towards Interactive Training with an Avatar-based Human-Computer Interface"'
date: 2008-12-01
tags: paper
categories: papers
tabs: true
image: iitsec_2008sm.jpg-srcw.jpg
---

## Towards Interactive Training with an Avatar-based Human-Computer Interface
**R. DeMara, Gonzalez, A., Hung, V., Leon-Barth, C., Dookhoo, R., Jones, S., Johnson, A., Leigh, J., Renambot, L., Lee, S., Carlson, G.**
- Publication: Orlando, Florida
- Publication: www.iitsec.org
- PDF: [avatar08.pdf](/documents/avatar08.pdf)


[![image](https://www.evl.uic.edu/output/originals/iitsec_2008sm.jpg-srcw.jpg){:style="max-width: 100%"}](https://www.evl.uic.edu/output/originals/iitsec_2008sm.jpg-srcw.jpg)
LifeLike System
Credit: S. Lee, EVL

The development of avatars has significant potential to enhance realism, automation capability, and effectiveness across a variety of training environments. Project Lifelike is a three-year National Science Foundation effort whose objective is to develop and evaluate realistic avatar interfaces as portals to intelligent programs capable of relaying knowledge and training skills. This interface aims towards support of spoken dialog within a limited domain and capabilities for learning to maintain its knowledge current and accurate. Research objectives focus on the integration of speaker-independent continuous speech recognition technology with a context-based dialog system and real-time graphics rendering capability derived from live subject motion capture traces. The motion capture traces are used by the avatar to provide spoken interaction with gestural expressions. This paper describes the first phase of the Lifelike project which developed an interactive avatar prototype of a National Science Foundation (NSF) program manager, Dr. Alex Schwarzkopf, for whom a contextual graph representation of domain knowledge was created. A Graphical Asset Production Pipeline was developed to allow digitization of the facial characteristics and physical movements of Dr. Schwarzkopf. Next, an example subset of his knowledge of NSF protocols was encoded in a grammar-based speech interpretation system and context-based reasoning system. These systems were integrated with the Lifelike Responsive Avatar Framework to enable the avatar to receive spoken input and generate appropriate verbal and non-verbal responses. The system demonstrates conveyance of knowledge within a limited domain such as NSF project reporting requirements. Work toward improving the realism of the avatar, long-term efforts toward creating a toolbox for generalization to other training applications, and results of evaluation of how users respond to different characteristics that contribute to realism in an avatar are discussed.