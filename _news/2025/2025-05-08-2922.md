---
layout: post
title: '"From Prompts to Priorities: Pairwise Learning-to-Rank Scheduling for Low-Latency  LLM Serving (poster)"'
date: 2025-05-08
tags: paper
categories: papers
tabs: true
image: llm_scheduling.png-srcw.jpg
---

## From Prompts to Priorities: Pairwise Learning-to-Rank Scheduling for Low-Latency  LLM Serving (poster)
**Tao, Y., Zhang, Y., Dearing, M. T., Wang, X., Lan, Z.**
- Publication: Chicago, IL
- Link: [https://gcasr.org/2025/posters](https://gcasr.org/2025/posters)
- PDF: [yiheng&jordan_llm_scheduling.pdf](/documents/yiheng&jordan_llm_scheduling.pdf)


[![image](https://www.evl.uic.edu/output/originals/llm_scheduling.png-srcw.jpg){:style="max-width: 100%"}](https://www.evl.uic.edu/output/originals/llm_scheduling.png-srcw.jpg)

Large language model (LLM) requests vary widely: a short factual query may produce only a few dozen tokens, while multi-step reasoning or proof generation can run to tens of thousands. Because generation is autoregressive, inference time grows proportionally with output length. Current serving stacks such as vLLM and Orca schedule requests using a simple first-come-first-serve (FCFS) policy. While fair, FCFS suffers from a well-known issue in LLM serving called head-of-line blocking, where a single long request delays all shorter ones behind it - leading to inflated tail latency and wasted throughput. A classical remedy is the Shortest-Job-First (SJF) policy, which improves efficiency by serving shorter jobs first. However, SJF requires knowledge of job length in advance - information that is unavailable in LLMs until generation completes, due to their autoregressive nature.<br><br>
We introduce a prompt-sensitive LLM task scheduler based on pairwise learning method from learning-to-rank utilizing Margin Ranking Loss and show that our method significantly improves scheduling performance with minimal overhead.