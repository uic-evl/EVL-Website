---
layout: post
title: '"Augmenting Small Data to Classify Contextualized Dialogue Acts for Exploratory Visualization"'
date: 2020-05-11
tags: paper
categories: papers
tabs: true
image: languageresources2020.png-srcw.jpg
---

## Augmenting Small Data to Classify Contextualized Dialogue Acts for Exploratory Visualization
**Kumar, A., Di Eugenio, B., Aurisano, J., Johnson, A.**
- Publication: Marseille, France
- Link: [https://www.aclweb.org/anthology/2020.lrec-1.74](https://www.aclweb.org/anthology/2020.lrec-1.74)
- PDF: [languageresources2020.pdf](/documents/languageresources2020.pdf)


[![image](https://www.evl.uic.edu/output/originals/languageresources2020.png-srcw.jpg){:style="max-width: 100%"}](https://www.evl.uic.edu/output/originals/languageresources2020.png-srcw.jpg)
Contextualized actionable requests segment relevant user utterances in each turn of the conversation.

Our goal is to develop an intelligent assistant to support users explore data via visualizations. We have collected a new corpus of conversations, CHICAGO-CRIME-VIS, geared towards supporting data visualization exploration, and we have annotated it for a variety of features, including contextualized dialogue acts. In this paper, we describe our strategies and their evaluation for dialogue act classification. We highlight how thinking aloud affects interpretation of dialogue acts in our setting and how to best capture that information. A key component of our strategy is data augmentation as applied to the training data, since our corpus is inherently small. We ran experiments with the Balanced Bagging Classifier (BAGC), Condiontal Random Field (CRF), and several Long Short Term Memory (LSTM) networks, and found that all of them improved compared to the baseline (e.g., without the data augmentation pipeline). CRF outperformed the other classification algorithms, with the LSTM networks showing modest improvement, even after obtaining a performance boost from domain-trained word embeddings. This result is of note because training a CRF is far less resource-intensive than training deep learning models, hence given a similar if not better performance, traditional methods may still be preferable in order to lower resource consumption.<br><br>
<strong>Keywords:</strong> Dialogue, Corpus, Statistical and Machine Learning Methods